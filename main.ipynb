{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from threading import Thread, Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde50d67-a117-4387-ae0f-93149ba1ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Project Configuration\n",
    "result_folder_dir = Path.cwd() / 'results'\n",
    "data_dir = Path.cwd() / 'data'\n",
    "\n",
    "# Learning Strategy Configuration\n",
    "# Options: 1 = naive fine-tuning (warm start), 2 = cumulative learning (from scratch), 3 = language based ER\n",
    "mode = 3\n",
    "\n",
    "# GPU ID to use for training\n",
    "GPU_ID = 1  \n",
    "\n",
    "# Energy Measurement Configuration\n",
    "power_measuring_mode = 0  # 0: no, 1: yes\n",
    "\n",
    "# Training Parameters\n",
    "num_days = 9  # Number of training days to simulate\n",
    "\n",
    "# Model Hyperparameters\n",
    "conf_th = 0.9        # Confidence threshold for YOLO inference\n",
    "num_epochs = 100     # Training epochs per day\n",
    "num_clusters = 20    # Number of clusters for language-based sampling\n",
    "buffer_size = 300    # Experience replay buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INITIALIZATION AND SETUP\n",
    "# =============================================================================\n",
    "# Initialize Performance Tracking Dictionaries\n",
    "daily_f1_scores = {}\n",
    "daily_precision_scores = {}\n",
    "daily_recall_scores = {}\n",
    "days_train_time = {}\n",
    "\n",
    "# Initialize Energy Tracking Dictionaries\n",
    "days_energy = {}\n",
    "\n",
    "# Initialize global variables for main experiment loop\n",
    "lan_accumulated_imgs_path = []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD LANGUAGE EMBEDDINGS\n",
    "# =============================================================================\n",
    "print(\"üìö Loading language embeddings...\")\n",
    "\n",
    "if mode == 3:\n",
    "    # Load precomputed embeddings and image description dictionaries\n",
    "    with open(f'{Path.cwd()}/Language_description_generator/text_embeddings_dict.pickle', 'rb') as file: # path to your text embeddings pickle file\n",
    "        text_embeddings_dict = pickle.load(file)\n",
    "    with open(f'{Path.cwd()}/Language_description_generator/image_descriptions_dict.pickle', 'rb') as file: # path to your image descriptions pickle file\n",
    "        image_descriptions_dict = pickle.load(file)\n",
    "\n",
    "    # Create image-to-embedding mapping\n",
    "    image_embeddings_dict = {}\n",
    "    for img_path, description in image_descriptions_dict.items():\n",
    "        if description in text_embeddings_dict:\n",
    "            image_embeddings_dict[img_path] = text_embeddings_dict[description]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Warning: No embedding found for description: {description}\")\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SETUP WORKING DIRECTORIES\n",
    "# =============================================================================\n",
    "print(\"üìÅ Setting up working directories...\")\n",
    "\n",
    "\n",
    "if mode == 3:\n",
    "    results_dir = f'{result_folder_dir}/language_based_ER'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    [shutil.rmtree(p) if os.path.isdir(p) else os.remove(p) for p in [os.path.join(results_dir, f) for f in os.listdir(results_dir)]]\n",
    "\n",
    "    # Setup language training buffer directory\n",
    "    experience_dir = f'{data_dir}/training_buffer_language'\n",
    "    experience_images = f'{experience_dir}/images'\n",
    "    experience_labels = f'{experience_dir}/labels'\n",
    "    \n",
    "    os.makedirs(experience_dir, exist_ok=True)\n",
    "    [shutil.rmtree(p) if os.path.isdir(p) else os.remove(p) for p in [os.path.join(experience_dir, f) for f in os.listdir(experience_dir)]]\n",
    "\n",
    "    # Create language training buffer directories if they don't exist\n",
    "    os.makedirs(experience_images, exist_ok=True)\n",
    "    os.makedirs(experience_labels, exist_ok=True)\n",
    "\n",
    "\n",
    "    \n",
    "elif mode == 1:  # Naive fine-tuning setup\n",
    "    results_dir = f'{result_folder_dir}/naive_fine_tuning'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    [shutil.rmtree(p) if os.path.isdir(p) else os.remove(p) for p in [os.path.join(results_dir, f) for f in os.listdir(results_dir)]]\n",
    "\n",
    "    dir_naive_train_temp = f'{data_dir}/training_buffer_naive_ft'\n",
    "    os.makedirs(dir_naive_train_temp, exist_ok=True)\n",
    "    [shutil.rmtree(p) if os.path.isdir(p) else os.remove(p) for p in [os.path.join(dir_naive_train_temp, f) for f in os.listdir(dir_naive_train_temp)]]\n",
    "\n",
    "    os.makedirs(os.path.join(dir_naive_train_temp, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dir_naive_train_temp, 'labels'), exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "elif mode == 2:  # Cumulative learning setup\n",
    "    results_dir = f'{result_folder_dir}/cumulative_learning'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    [shutil.rmtree(p) if os.path.isdir(p) else os.remove(p) for p in [os.path.join(results_dir, f) for f in os.listdir(results_dir)]]\n",
    "\n",
    "    cumulative_buffer_dir = f'{data_dir}/training_buffer_cumulative'\n",
    "    os.makedirs(cumulative_buffer_dir, exist_ok=True)\n",
    "    [shutil.rmtree(p) if os.path.isdir(p) else os.remove(p) for p in [os.path.join(cumulative_buffer_dir, f) for f in os.listdir(cumulative_buffer_dir)]]\n",
    "\n",
    "\n",
    "    os.makedirs(os.path.join(cumulative_buffer_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cumulative_buffer_dir, 'labels'), exist_ok=True)\n",
    "\n",
    "# Initialize energy tracking on first run or load existing data\n",
    "if power_measuring_mode == 1:\n",
    "    energy_file_path = f'{results_dir}/days_energy.pkl'\n",
    "    if os.path.exists(energy_file_path):\n",
    "        with open(energy_file_path, 'rb') as file:\n",
    "            days_energy = pickle.load(file)\n",
    "        print(f\"üìä Loaded existing energy data: {days_energy}\")\n",
    "    else:\n",
    "        days_energy = {}\n",
    "        print(\"üìä Initialized new energy tracking\")\n",
    "\n",
    "# Clean up YOLO's default runs directory if it exists\n",
    "if os.path.exists('runs') and os.path.isdir('runs'):\n",
    "    shutil.rmtree('runs')  # Prevents clutter from YOLO outputs\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INITIAL YOLOMODEL PREPARATION\n",
    "# =============================================================================\n",
    "print(\"üöÄ Initializing base YOLO model...\")\n",
    "yolo = YOLO('yolov8n.pt')\n",
    "init_results =yolo.train(\n",
    "    data=f'{data_dir}/day1/data.yaml', \n",
    "    epochs=1, \n",
    "    device=1,  # Use both GPUs\n",
    "    project=results_dir, \n",
    "    name=f'yolo_checkpoints/day0', \n",
    "    lr0=0.0000000000001, \n",
    "    lrf=0.0000000000001, \n",
    "    optimizer='Adam', \n",
    "    verbose=False  # Suppress detailed training output\n",
    ");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2695ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENERGY MEASUREMENT FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Global flag for emergency stop\n",
    "energy_monitoring_stopped = False\n",
    "\n",
    "# Function to measure the total power consumption of GPUs during training\n",
    "def log_gpu_power_during_training(interval=1.0, stop_event=None, idle_power=0, energy_container=None):\n",
    "    global energy_monitoring_stopped\n",
    "    total_energy = 0.0  # To accumulate the total energy consumed\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        while not stop_event.is_set() and not energy_monitoring_stopped:  # Check both stop conditions\n",
    "            try:\n",
    "                # Check for global stop flag frequently\n",
    "                if energy_monitoring_stopped:\n",
    "                    print(\"Energy monitoring stopped by global flag\")\n",
    "                    break\n",
    "                    \n",
    "                gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'], \n",
    "                                          stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)\n",
    "                power_draws = [float(power) - idle_power for power in gpu_result.stdout.decode('utf-8').strip().split('\\n')]\n",
    "                power_draws = [power_draws[GPU_ID]]\n",
    "                # Assuming the power_draws list has power readings for both GPUs\n",
    "                avg_power = sum(power_draws) / len(power_draws)\n",
    "                # Calculate the elapsed time since the last measurement\n",
    "                elapsed_time = interval  \n",
    "                \n",
    "                # Accumulate energy consumed: Energy (Joules) = Power (Watts) * Time (Seconds)\n",
    "                total_energy += 1 * avg_power * elapsed_time\n",
    "                \n",
    "                print(f\"# Time: {time.time() - start_time:.2f} seconds, GPU Power Consumption (Net): {power_draws} Watts\")\n",
    "                \n",
    "                for i in range(int(interval * 10)):  # Check every 0.1 seconds\n",
    "                    if stop_event.is_set() or energy_monitoring_stopped:\n",
    "                        break\n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "            except (subprocess.TimeoutExpired, subprocess.CalledProcessError, ValueError, IndexError) as e:\n",
    "                print(f\"Warning: Error reading GPU power: {e}\")\n",
    "                if stop_event.wait(interval) or energy_monitoring_stopped:  # Wait for interval or until stop event\n",
    "                    break\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Energy monitoring interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Energy monitoring stopped due to error: {e}\")\n",
    "    finally:\n",
    "        if energy_container is not None:\n",
    "            energy_container.append(total_energy)\n",
    "        print(f\"Energy monitoring thread stopped. Total energy: {total_energy:.2f} Joules\")\n",
    "\n",
    "# Measure idle power before starting the training\n",
    "def measure_idle_gpu_power(duration=10, interval=1.0):\n",
    "    global energy_monitoring_stopped\n",
    "    idle_power = []\n",
    "    try:\n",
    "        for _ in range(int(duration / interval)):\n",
    "            if energy_monitoring_stopped:\n",
    "                break\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'], \n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)\n",
    "            power_draws = [float(power) for power in result.stdout.decode('utf-8').strip().split('\\n')]\n",
    "            power_draws = [power_draws[GPU_ID]] # to track only one GPU\n",
    "            idle_power.append(sum(power_draws) / len(power_draws)) \n",
    "            time.sleep(interval)\n",
    "    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, ValueError, IndexError) as e:\n",
    "        print(f\"Warning: Error measuring idle power: {e}\")\n",
    "        return 0  # Return 0 if measurement fails\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Idle power measurement interrupted\")\n",
    "        if idle_power:\n",
    "            return sum(idle_power) / len(idle_power)\n",
    "        return 0\n",
    "        \n",
    "    return sum(idle_power) / len(idle_power) if idle_power else 0 \n",
    "\n",
    "def measure_gpu_power(duration=10, interval=1):\n",
    "    global energy_monitoring_stopped\n",
    "    power_measurements = []\n",
    "    gpu_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        while (time.time() - gpu_start_time) < duration and not energy_monitoring_stopped:\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'], \n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)\n",
    "            power_draws = [float(power) for power in result.stdout.decode('utf-8').strip().split('\\n')]\n",
    "            power_draws = [power_draws[GPU_ID]] # to track only one GPU\n",
    "            power_measurements.append(power_draws)\n",
    "            print(power_draws)\n",
    "            \n",
    "            time.sleep(interval)\n",
    "    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, ValueError, IndexError) as e:\n",
    "        print(f\"Warning: Error measuring GPU power: {e}\")\n",
    "        if not power_measurements:\n",
    "            return [0]  # Return default if no measurements\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"GPU power measurement interrupted\")\n",
    "    \n",
    "    if not power_measurements:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate average power consumption \n",
    "    try:\n",
    "        avg_power_per_gpu = [sum(gpu_power) / len(gpu_power) for gpu_power in zip(*power_measurements)]\n",
    "        return avg_power_per_gpu\n",
    "    except:\n",
    "        return [sum(sum(measurement) for measurement in power_measurements) / len(power_measurements)]\n",
    "    \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENERGY MONITORING CLEANUP UTILITY\n",
    "# =============================================================================\n",
    "\n",
    "import threading\n",
    "'''\n",
    "def stop_all_energy_threads():\n",
    "    \"\"\"\n",
    "    Utility function to stop any lingering energy monitoring threads\n",
    "    Call this if energy monitoring continues after cell interruption\n",
    "    \"\"\"\n",
    "    print(\"üõë Stopping all energy monitoring threads...\")\n",
    "    \n",
    "    for thread in threading.enumerate():\n",
    "        if thread.name != 'MainThread' and 'log_gpu_power' in str(thread._target) if hasattr(thread, '_target') else False:\n",
    "            print(f\"Found energy monitoring thread: {thread.name}\")\n",
    "            if hasattr(thread, '_stop'):\n",
    "                thread._stop()\n",
    "    \n",
    "    print(\"‚úÖ Energy monitoring cleanup complete\")\n",
    "\n",
    "current_stop_event = None\n",
    "current_energy_thread = None\n",
    "\n",
    "def cleanup_energy_monitoring():\n",
    "    \"\"\"Clean up current energy monitoring thread\"\"\"\n",
    "    global current_stop_event, current_energy_thread\n",
    "    \n",
    "    if current_stop_event is not None:\n",
    "        print(\"Stopping energy monitoring...\")\n",
    "        current_stop_event.set()\n",
    "        \n",
    "    if current_energy_thread is not None and current_energy_thread.is_alive():\n",
    "        current_energy_thread.join(timeout=3)\n",
    "\n",
    "        \n",
    "    current_stop_event = None\n",
    "    current_energy_thread = None\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# MEASURE IDLE GPU POWER\n",
    "# =============================================================================\n",
    "if power_measuring_mode == 1:\n",
    "    print(\"üîå Measuring idle GPU power consumption...\")\n",
    "    # Measure GPU power for 10 seconds and get idle power baseline\n",
    "    avg_power = measure_gpu_power(duration=10, interval=1)\n",
    "    for i, avg_power_gpu in enumerate(avg_power):\n",
    "        print(f\"Average Power Consumption for GPU {i}: {avg_power_gpu:.2f} Watts\")\n",
    "\n",
    "    idle_power = sum(avg_power)/len(avg_power)\n",
    "    print(f'idle_power: {idle_power:.3f} Watts')\n",
    "else:\n",
    "    idle_power = 0\n",
    "    print(\"‚ö†Ô∏è Power measurement disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f02b7-364b-422f-ad93-fc91c0b10e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXPERIMENT LOOP - INCREMENTAL LEARNING SIMULATION\n",
    "# =============================================================================\n",
    "\n",
    "for day_number in range(1, num_days + 1):\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DAILY MODEL INITIALIZATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Load the model from previous day's training\n",
    "    yolo = YOLO(f'{results_dir}/yolo_checkpoints/day{day_number-1}/weights/best.pt')\n",
    "\n",
    "    # Load accumulated performance metrics from previous days (if any)\n",
    "    if day_number > 1:\n",
    "        with open(f'{results_dir}/daily_precision_scores.pkl', 'rb') as file:\n",
    "            daily_precision_scores = pickle.load(file)\n",
    "        with open(f'{results_dir}/daily_recall_scores.pkl', 'rb') as file:\n",
    "            daily_recall_scores = pickle.load(file)\n",
    "        with open(f'{results_dir}/daily_f1_scores.pkl', 'rb') as file:\n",
    "            daily_f1_scores = pickle.load(file)\n",
    "        with open(f'{results_dir}/days_train_time.pkl', 'rb') as file:\n",
    "            days_train_time = pickle.load(file)\n",
    "        \n",
    "        # Load energy data if power measurement is enabled\n",
    "        if power_measuring_mode == 1:\n",
    "            with open(f'{results_dir}/days_energy.pkl', 'rb') as file:\n",
    "                days_energy = pickle.load(file)\n",
    "    else:\n",
    "        # Initialize metrics dictionaries for first day\n",
    "        daily_precision_scores = {}\n",
    "        daily_recall_scores = {}\n",
    "        daily_f1_scores = {}\n",
    "        days_train_time = {}\n",
    "        \n",
    "        # Initialize energy tracking\n",
    "        if power_measuring_mode == 1:\n",
    "            days_energy = {}\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODEL EVALUATION ON CURRENT DAY\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(f\"\\n\\nüî¨ Day {day_number} - Model Evaluation\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    yolo_conf_paths = []  # Store paths of images where YOLO failed to detect the action\n",
    "    \n",
    "    # Prepare test data for current day\n",
    "    test_day_path = f'{data_dir}/day{day_number}/images'\n",
    "    imgs_path = []\n",
    "    for path in os.listdir(test_day_path):\n",
    "        imgs_path.append(path)\n",
    "    imgs_path.sort()\n",
    "    \n",
    "    # Create directory for saving performance visualizations\n",
    "    os.makedirs(f\"{results_dir}/vacuum_performance/day{day_number}\", exist_ok=True)\n",
    "    \n",
    "    # Run inference on each test image and save results\n",
    "    for img_idx in range(0, len(imgs_path)):\n",
    "        source = f'{test_day_path}/{imgs_path[img_idx]}'\n",
    "        results = yolo(source, conf=conf_th, verbose=False)  # Suppress inference output\n",
    "        \n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            # Collect images where YOLO failed to detect any objects\n",
    "            if boxes.cls.numel() == 0:\n",
    "                yolo_conf_paths.append(f'{test_day_path}/{imgs_path[img_idx]}')\n",
    "            \n",
    "            # Save visualization of detection results\n",
    "            result.save(filename=f\"{results_dir}/vacuum_performance/day{day_number}/{imgs_path[img_idx]}\")\n",
    "\n",
    "    # Formal validation and metric computation\n",
    "    yolo_test_on_day = yolo.val(data=f'{data_dir}/day{day_number}/data.yaml', split='test', conf=conf_th, verbose=False)  # Suppress validation output\n",
    "    dict_metrics = yolo_test_on_day.results_dict   \n",
    "    \n",
    "    # Calculate F1 score\n",
    "    precision = dict_metrics['metrics/precision(B)']\n",
    "    recall = dict_metrics['metrics/recall(B)']\n",
    "    f1 = 2 * (precision * recall) / (0.001 + precision + recall)\n",
    "    \n",
    "    # Store metrics for current day\n",
    "    daily_precision_scores[f'day{day_number}'] = precision\n",
    "    daily_recall_scores[f'day{day_number}'] = recall\n",
    "    daily_f1_scores[f'day{day_number}'] = f1\n",
    "    \n",
    "    # Display performance metrics\n",
    "    print(f\"\\n\\nüìä Performance Metrics:\")\n",
    "    print(f\"   Precision: {daily_precision_scores[f'day{day_number}']:.4f}\")\n",
    "    print(f\"   Recall:    {daily_recall_scores[f'day{day_number}']:.4f}\")\n",
    "    print(f\"   F1 Score:  {daily_f1_scores[f'day{day_number}']:.4f}\")\n",
    "\n",
    "    # Save metrics to files\n",
    "    with open(f'{results_dir}/daily_precision_scores.pkl', 'wb') as file:\n",
    "        pickle.dump(daily_precision_scores, file)\n",
    "    with open(f'{results_dir}/daily_recall_scores.pkl', 'wb') as file:\n",
    "        pickle.dump(daily_recall_scores, file)\n",
    "    with open(f'{results_dir}/daily_f1_scores.pkl', 'wb') as file:\n",
    "        pickle.dump(daily_f1_scores, file)\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRAINING STRATEGY SELECTION AND EXECUTION\n",
    "    # =========================================================================\n",
    "\n",
    "    if mode == 1:  # Naive Fine-Tuning with Warm Start\n",
    "        print(f\"\\n\\nüéØ Day {day_number} - Naive Fine-Tuning Training\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        # Prepare training data using only failed detection images\n",
    "        train_images_dir = f'{dir_naive_train_temp}/images'\n",
    "        train_labels_dir = f'{dir_naive_train_temp}/labels'\n",
    "        \n",
    "        # Clean previous training data\n",
    "        [os.remove(os.path.join(train_images_dir, f)) for f in os.listdir(train_images_dir) if os.path.isfile(os.path.join(train_images_dir, f))]\n",
    "        [os.remove(os.path.join(train_labels_dir, f)) for f in os.listdir(train_labels_dir) if os.path.isfile(os.path.join(train_labels_dir, f))]\n",
    "\n",
    "        # Copy failed detection images and labels to training directory\n",
    "        for image_name in yolo_conf_paths:\n",
    "            label_name = image_name.replace('images', 'labels').replace('jpg', 'txt')\n",
    "            shutil.copy(image_name, train_images_dir)\n",
    "            shutil.copy(label_name, train_labels_dir)\n",
    "\n",
    "        # Create validation set (20% of training data)\n",
    "        sample_size = max(1, int(len(yolo_conf_paths) * 0.2))\n",
    "        val_list_images = random.sample(yolo_conf_paths, sample_size)\n",
    "\n",
    "        val_images_dir = f'{data_dir}/ValFolder/images'\n",
    "        val_labels_dir = f'{data_dir}/ValFolder/labels'\n",
    "        \n",
    "        # Clean validation directories\n",
    "        [os.remove(os.path.join(val_images_dir, f)) for f in os.listdir(val_images_dir) if os.path.isfile(os.path.join(val_images_dir, f))]\n",
    "        [os.remove(os.path.join(val_labels_dir, f)) for f in os.listdir(val_labels_dir) if os.path.isfile(os.path.join(val_labels_dir, f))]\n",
    "\n",
    "        # Copy validation images and labels\n",
    "        for image_name in val_list_images:\n",
    "            label_name = image_name.replace('images', 'labels').replace('jpg', 'txt')\n",
    "            shutil.copy(image_name, val_images_dir)\n",
    "            shutil.copy(label_name, val_labels_dir)\n",
    "\n",
    "        # Create YAML configuration for training\n",
    "        datayaml = {\n",
    "            'train': train_images_dir, \n",
    "            'val': val_images_dir,\n",
    "            'test': val_images_dir,\n",
    "            'nc': 2,\n",
    "            'names': ['avoid', 'suck']\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(dir_naive_train_temp, 'data.yaml'), 'w') as yaml_file:\n",
    "            yaml.dump(datayaml, yaml_file, default_flow_style=None)\n",
    "\n",
    "        YAML_DIR = f'{dir_naive_train_temp}/data.yaml'\n",
    "\n",
    "    elif mode == 2:  # Cumulative Learning from Scratch\n",
    "        print(f\"\\n\\nüìö Day {day_number} - Cumulative Learning Training\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Initialize cumulative training setup on first day\n",
    "        if day_number == 1:\n",
    "            accumulated_images = f'{data_dir}/training_buffer_cumulative/images'\n",
    "            accumulated_labels = f'{data_dir}/training_buffer_cumulative/labels'\n",
    "            val_images = f'{data_dir}/ValFolder/images'\n",
    "            \n",
    "            datayaml = {\n",
    "                'train': accumulated_images,\n",
    "                'val': val_images,\n",
    "                'test': accumulated_images,\n",
    "                'nc': 2,\n",
    "                'names': ['avoid', 'suck']\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(f'{data_dir}/training_buffer_cumulative', 'data.yaml'), 'w') as yaml_file:\n",
    "                yaml.dump(datayaml, yaml_file, default_flow_style=None)\n",
    "\n",
    "        # Reset to base model for training from scratch\n",
    "        yolo = YOLO('yolov8n.pt')\n",
    "        YAML_DIR = f'{data_dir}/training_buffer_cumulative/data.yaml'\n",
    "\n",
    "        # Add current day's failed detections to accumulated dataset\n",
    "        accumulated_images = f'{data_dir}/training_buffer_cumulative/images'\n",
    "        accumulated_labels = f'{data_dir}/training_buffer_cumulative/labels'\n",
    "\n",
    "        for image_name in yolo_conf_paths:\n",
    "            label_name = image_name.replace('images', 'labels').replace('jpg', 'txt')\n",
    "            shutil.copy(image_name, accumulated_images)\n",
    "            shutil.copy(label_name, accumulated_labels) \n",
    "\n",
    "        # Create validation set from accumulated data\n",
    "        imgs_path_val_select = []\n",
    "        for path in os.listdir(accumulated_images):\n",
    "            imgs_path_val_select.append(os.path.join(accumulated_images, path))\n",
    "\n",
    "        sample_size = max(1, int(len(imgs_path_val_select) * 0.2))\n",
    "        val_list_images = random.sample(imgs_path_val_select, sample_size)\n",
    "\n",
    "        val_images_dir = f'{data_dir}/ValFolder/images'\n",
    "        val_labels_dir = f'{data_dir}/ValFolder/labels'\n",
    "        \n",
    "        # Clean and populate validation directories\n",
    "        [os.remove(os.path.join(val_images_dir, f)) for f in os.listdir(val_images_dir) if os.path.isfile(os.path.join(val_images_dir, f))]\n",
    "        [os.remove(os.path.join(val_labels_dir, f)) for f in os.listdir(val_labels_dir) if os.path.isfile(os.path.join(val_labels_dir, f))]\n",
    "\n",
    "        for image_name in val_list_images:\n",
    "            label_name = image_name.replace('images', 'labels').replace('jpg', 'txt')\n",
    "            shutil.copy(image_name, val_images_dir)\n",
    "            shutil.copy(label_name, val_labels_dir)\n",
    "\n",
    "    elif mode == 3:  # Language-Based Experience Replay\n",
    "        print(f\"\\n\\nüî§ Day {day_number} - Language-Based Experience Replay\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Setup clustering directory for current day\n",
    "        os.makedirs(f'{results_dir}/clusters/day{day_number}', exist_ok=True)\n",
    "        dir_clusters = f'{results_dir}/clusters/day{day_number}'\n",
    "        [os.remove(os.path.join(dir_clusters, f)) for f in os.listdir(dir_clusters) if os.path.isfile(os.path.join(dir_clusters, f))]\n",
    "   \n",
    "        # Accumulate failed detection images across all days\n",
    "        lan_accumulated_imgs_path.extend(yolo_conf_paths)\n",
    "\n",
    "        # Create embeddings dictionary for accumulated images\n",
    "        # Convert absolute paths to relative paths for lookup in image_embeddings_dict\n",
    "        image_to_embedding = {}\n",
    "        for img_path in lan_accumulated_imgs_path:\n",
    "            # Convert absolute path to relative path by removing the workspace directory\n",
    "            relative_img_path = os.path.relpath(img_path, Path.cwd())\n",
    "            if relative_img_path in image_embeddings_dict:\n",
    "                image_to_embedding[img_path] = image_embeddings_dict[relative_img_path]\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Warning: No embedding found for image: {relative_img_path}\")\n",
    "\n",
    "        # Prepare data for K-means clustering\n",
    "        image_names = list(image_to_embedding.keys())\n",
    "        embeddings = list(image_to_embedding.values())\n",
    "\n",
    "        # Perform K-means clustering on language embeddings\n",
    "        mini_kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "        mini_kmeans.fit(embeddings)\n",
    "        labels = mini_kmeans.labels_\n",
    "\n",
    "        # Organize images by cluster\n",
    "        cluster_to_images_dict = defaultdict(list)\n",
    "        for image_name, label in zip(image_names, labels):\n",
    "            cluster_to_images_dict[label].append(image_name)\n",
    "        \n",
    "        # Build experience replay buffer\n",
    "        experience_buffer = []\n",
    "        for cluster_label, image_list in cluster_to_images_dict.items():\n",
    "            \n",
    "            # Save cluster images for analysis\n",
    "            for cluster_img_path in image_list:\n",
    "                cluster_img_name = os.path.basename(cluster_img_path)\n",
    "                cluster_dest_path = os.path.join(f'{results_dir}/clusters/day{day_number}', f'L{cluster_label}_{cluster_img_name}')\n",
    "                shutil.copy(cluster_img_path, cluster_dest_path)\n",
    "\n",
    "            # Sample from cluster for experience buffer\n",
    "            if len(image_list) >= buffer_size // num_clusters:\n",
    "                selected_images = random.sample(image_list, buffer_size // num_clusters)\n",
    "            else:\n",
    "                selected_images = image_list\n",
    "            experience_buffer.extend(selected_images)\n",
    "\n",
    "\n",
    "        \n",
    "        # Clean language training buffer directories\n",
    "        for directory in [experience_images, experience_labels]:\n",
    "            for filename in os.listdir(directory):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        \n",
    "        # Copy experience buffer to training directory\n",
    "        for image_path in experience_buffer:\n",
    "            label_path = image_path.replace('images', 'labels').replace('jpg', 'txt')\n",
    "            shutil.copy(image_path, experience_images)\n",
    "            shutil.copy(label_path, experience_labels)\n",
    "        \n",
    "        # Save experience buffer for analysis\n",
    "        save_experience_images = f'{results_dir}/experience_buffer/day{day_number}'\n",
    "        os.makedirs(save_experience_images, exist_ok=True)\n",
    "        \n",
    "        # Clean and populate language training buffer archive\n",
    "        for filename in os.listdir(save_experience_images):\n",
    "            file_path = os.path.join(save_experience_images, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        for buffer_image_path in experience_buffer:\n",
    "            shutil.copy(buffer_image_path, save_experience_images)\n",
    "\n",
    "        # Create validation set from experience buffer\n",
    "        sample_size = max(1, int(len(experience_buffer) * 0.2))\n",
    "        val_list_images = random.sample(experience_buffer, sample_size)\n",
    "        \n",
    "        val_images_dir = f'{data_dir}/ValFolder/images'\n",
    "        val_labels_dir = f'{data_dir}/ValFolder/labels'\n",
    "        \n",
    "        # Clean and populate validation directories\n",
    "        for directory in [val_images_dir, val_labels_dir]:\n",
    "            for filename in os.listdir(directory):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        \n",
    "        for image_path in val_list_images:\n",
    "            label_path = image_path.replace('images', 'labels').replace('jpg', 'txt')\n",
    "            shutil.copy(image_path, val_images_dir)\n",
    "            shutil.copy(label_path, val_labels_dir)\n",
    "\n",
    "        # Create YAML configuration for language training buffer\n",
    "        datayaml = {\n",
    "            'train': f'{experience_dir}/images', \n",
    "            'val': val_images_dir,\n",
    "            'test': f'{experience_dir}/images',\n",
    "            'nc': 2,\n",
    "            'names': ['avoid', 'suck']\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(experience_dir, 'data.yaml'), 'w') as yaml_file:\n",
    "            yaml.dump(datayaml, yaml_file, default_flow_style=None)\n",
    "\n",
    "        YAML_DIR = f'{experience_dir}/data.yaml'\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODEL TRAINING EXECUTION WITH ENERGY MEASUREMENT\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Clean up any existing training directory for current day\n",
    "    check_dir = f'{results_dir}/day{day_number}'\n",
    "    if os.path.exists(check_dir) and os.path.isdir(check_dir):\n",
    "        shutil.rmtree(check_dir)\n",
    "\n",
    "    # Initialize variables for energy measurement\n",
    "    stop_event = None\n",
    "    energy_container = []\n",
    "    log_thread = None\n",
    "    \n",
    "    try:\n",
    "        # Start energy measurement if enabled\n",
    "        if power_measuring_mode == 1:\n",
    "            stop_event = Event()\n",
    "            energy_container = []\n",
    "            log_thread = Thread(target=log_gpu_power_during_training, args=(1.0, stop_event, idle_power, energy_container), daemon=False)\n",
    "            log_thread.start()\n",
    "\n",
    "        # Execute YOLO training with configured parameters\n",
    "        train_start_time = time.time()\n",
    "        yolo.train(\n",
    "            data=YAML_DIR, \n",
    "            epochs=num_epochs, \n",
    "            project=results_dir, \n",
    "            name=f'yolo_checkpoints/day{day_number}', \n",
    "            device=1, \n",
    "            patience=200, \n",
    "            verbose=False  # Suppress detailed training output\n",
    "        )\n",
    "        train_end_time = time.time()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è Training interrupted by user on day {day_number}\")\n",
    "        train_end_time = time.time()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed on day {day_number}: {e}\")\n",
    "        train_end_time = time.time()\n",
    "        \n",
    "    finally:\n",
    "        # Always ensure energy measurement thread is stopped properly\n",
    "        if power_measuring_mode == 1 and stop_event is not None:\n",
    "            print(\"Stopping energy measurement thread...\")\n",
    "            stop_event.set()\n",
    "            if log_thread is not None and log_thread.is_alive():\n",
    "                log_thread.join(timeout=5)  # Wait up to 5 seconds for thread to stop\n",
    "                if log_thread.is_alive():\n",
    "                    print(\"‚ö†Ô∏è Energy monitoring thread did not stop gracefully\")\n",
    "            \n",
    "            total_energy = energy_container[0] if energy_container else 0\n",
    "            days_energy[f'day{day_number}'] = total_energy\n",
    "            print(f'   ‚ö° Energy consumed on day {day_number}: {total_energy:.2f} Joules')\n",
    "            print('   ##############   days_energy: ', days_energy)\n",
    "\n",
    "            with open(f'{results_dir}/days_energy.pkl', 'wb') as file:\n",
    "                pickle.dump(days_energy, file)\n",
    "\n",
    "        # Record training time\n",
    "        if 'train_start_time' in locals():\n",
    "            days_train_time[f'day{day_number}'] = (train_end_time - train_start_time)/60\n",
    "            with open(f'{results_dir}/days_train_time.pkl', 'wb') as file:\n",
    "                pickle.dump(days_train_time, file)\n",
    "            print(f'   ‚è±Ô∏è  Training time for day {day_number}: {days_train_time[f\"day{day_number}\"]:.2f} minutes')\n",
    "\n",
    "\n",
    "print(\"\\n\\nüéâ Experiment Complete!\")\n",
    "if power_measuring_mode == 1:\n",
    "    print(\"üìä Final Energy Consumption Summary:\")\n",
    "    for day, energy in days_energy.items():\n",
    "        print(f\"   {day}: {energy:.2f} Joules\")\n",
    "    total_energy_consumed = sum(days_energy.values())\n",
    "    print(f\"   Total Energy: {total_energy_consumed:.2f} Joules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa03c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 scores for all available modes\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Define mode configurations in desired order: naive, language, cumulative\n",
    "mode_configs = {\n",
    "    1: {'name': 'naive fine-tuning', 'color': 'blue', 'path': Path.cwd() / 'results/naive_fine_tuning/daily_f1_scores.pkl', 'marker': 'o'},\n",
    "    3: {'name': 'language-based ER', 'color': 'green', 'path': Path.cwd() / 'results/language_based_ER/daily_f1_scores.pkl', 'marker': '^'},\n",
    "    2: {'name': 'cumulative learning', 'color': 'orange', 'path': Path.cwd() / 'results/cumulative_learning/daily_f1_scores.pkl', 'marker': 's'}\n",
    "}\n",
    "\n",
    "# Check which modes have data available\n",
    "available_modes = []\n",
    "for mode_num, config in mode_configs.items():\n",
    "    if os.path.exists(config['path']):\n",
    "        available_modes.append(mode_num)\n",
    "\n",
    "# Print unavailable modes first\n",
    "unavailable_modes = [mode_configs[m]['name'] for m in [1, 3, 2] if m not in available_modes]\n",
    "if unavailable_modes:\n",
    "    for mode_name in unavailable_modes:\n",
    "        print(f\"{mode_name} dict not available\")\n",
    "\n",
    "if available_modes:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    # Collect all unique days across available modes\n",
    "    all_days = set()\n",
    "    for mode_num in available_modes:\n",
    "        with open(mode_configs[mode_num]['path'], 'rb') as file:\n",
    "            daily_f1_scores = pickle.load(file)\n",
    "            all_days.update(daily_f1_scores.keys())\n",
    "    \n",
    "    days = sorted(list(all_days))\n",
    "    x = np.arange(len(days))\n",
    "    \n",
    "    # Add shading to separate sets of three days\n",
    "    cc = 0\n",
    "    for i in range(0, len(days), 3):\n",
    "        a = [6/60, 3/60, 0/60]\n",
    "        ax.axvspan(i - 0.5, i + 2.5, color='red', alpha=a[cc])\n",
    "        cc += 1\n",
    "    \n",
    "    # Plot lines for each available mode and calculate mean for last 6 days\n",
    "    print(\"\\nMean F1 score for last 6 days:\")\n",
    "    for mode_num in available_modes:\n",
    "        config = mode_configs[mode_num]\n",
    "        \n",
    "        with open(config['path'], 'rb') as file:\n",
    "            daily_f1_scores = pickle.load(file)\n",
    "        \n",
    "        # Align F1 values with all_days\n",
    "        f1_values = [daily_f1_scores.get(day, 0) for day in days]\n",
    "        \n",
    "        ax.plot(x, f1_values, marker=config['marker'], linestyle='-', label=config['name'], \n",
    "                markersize=4, linewidth=1, color=config['color'])\n",
    "        \n",
    "        # Calculate mean F1 for last 6 days\n",
    "        if len(f1_values) >= 6:\n",
    "            last_6_f1 = f1_values[-6:]\n",
    "            mean_f1 = np.mean(last_6_f1)\n",
    "            print(f\"  {config['name']}: {mean_f1:.4f}\")\n",
    "        else:\n",
    "            mean_f1 = np.mean(f1_values) if f1_values else 0\n",
    "            print(f\"  {config['name']}: {mean_f1:.4f} (only {len(f1_values)} days available)\")\n",
    "    \n",
    "    # Add labels and formatting\n",
    "    ax.set_ylabel('F$_1$ score')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(days)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.grid(True, which='both', linestyle='--', color='lightgray', linewidth=0.5)\n",
    "    ax.grid(axis='x', color='gray', linewidth=0.7)\n",
    "    ax.minorticks_on()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No F1 score data files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mode configurations\n",
    "mode_configs = {\n",
    "    1: {'name': 'Naive FT (warm start)', 'color': '#1f77b4', 'path': Path.cwd() / 'results/naive_fine_tuning/days_energy.pkl'},  # blue\n",
    "    3: {'name': 'Language-based ER', 'color': '#2ca02c', 'path': Path.cwd() / 'results/language_based_ER/days_energy.pkl'},      # green\n",
    "    2: {'name': 'Cumulative learning', 'color': '#ff7f0e', 'path': Path.cwd() / 'results/cumulative_learning/days_energy.pkl'}   # orange\n",
    "}\n",
    "\n",
    "# Check available data files\n",
    "available_modes = [m for m, cfg in mode_configs.items() if os.path.exists(cfg['path'])]\n",
    "\n",
    "# Optional: compute energy reduction\n",
    "if 3 in available_modes and 2 in available_modes:\n",
    "    with open(mode_configs[3]['path'], 'rb') as f:\n",
    "        language_energy = pickle.load(f)\n",
    "    with open(mode_configs[2]['path'], 'rb') as f:\n",
    "        cumulative_energy = pickle.load(f)\n",
    "    total_lang = sum(language_energy.values())\n",
    "    total_cum = sum(cumulative_energy.values())\n",
    "    if total_cum > 0:\n",
    "        reduction = ((total_cum - total_lang) / total_cum) * 100\n",
    "        print(f\"Energy reduction (Language-based ER vs Cumulative): {reduction:.1f}%\")\n",
    "        print(f\"Language total: {total_lang:.2f} J, Cumulative total: {total_cum:.2f} J\")\n",
    "\n",
    "# Plot\n",
    "if available_modes:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Clean grid and background\n",
    "    ax.set_facecolor('white')\n",
    "    ax.grid(True, which='major', linestyle='--', color='lightgray', linewidth=0.8)\n",
    "    ax.grid(True, which='minor', linestyle='--', color='lightgray', linewidth=0.5)\n",
    "    ax.minorticks_on()\n",
    "\n",
    "    # Collect days across modes\n",
    "    all_days = set()\n",
    "    for m in available_modes:\n",
    "        with open(mode_configs[m]['path'], 'rb') as f:\n",
    "            all_days.update(pickle.load(f).keys())\n",
    "    days = sorted(list(all_days))\n",
    "    x = np.arange(len(days))\n",
    "\n",
    "    bar_width = 0.25 if len(available_modes) > 1 else 0.6\n",
    "    max_energy = 0\n",
    "\n",
    "    for idx, m in enumerate(available_modes):\n",
    "        cfg = mode_configs[m]\n",
    "        with open(cfg['path'], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        values = np.array([data.get(day, 0) for day in days]) / 1000\n",
    "        max_energy = max(max_energy, np.max(values))\n",
    "        if len(available_modes) > 1:\n",
    "            xpos = x + (idx - len(available_modes)/2 + 0.5) * bar_width\n",
    "        else:\n",
    "            xpos = x\n",
    "        ax.bar(xpos, values, width=bar_width, label=cfg['name'], color=cfg['color'], zorder=3)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(days)\n",
    "    ax.set_ylabel('GPU energy consumption (kJ)')\n",
    "    ax.set_ylim(0, max_energy * 1.1)\n",
    "    ax.legend(frameon=False)  # legend without border\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No energy data files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54c32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc0461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49442e-903c-4030-ba27-cc51b63dbb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dcd8c0-3542-4ca9-9789-f7b90dbef1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0e637-6a3c-4a0a-a681-463bca0f6ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b172d6a-8aee-4621-8283-3aab402cb4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7bc78-2d8a-4f1f-9c6d-1857cb4c6649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c0c51-33f5-49da-b25e-9fd475e5d151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e4800-632a-41d6-a26c-f0a3280d7cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c414289-2149-406f-9707-0a31c768a524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6237d-7445-464b-8412-c3cf1d67e315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLM-Vac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
