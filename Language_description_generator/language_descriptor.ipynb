{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cd649-f618-40a0-8bca-3d126c31849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Import Libraries\n",
    "# ========================\n",
    "import os\n",
    "import time \n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Audio, Markdown\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from transformers.image_utils import ImageFeatureExtractionMixin\n",
    "from transformers.utils import send_example_telemetry\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from torchvision.datasets import CIFAR100\n",
    "from openai import OpenAI \n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cd649-f618-40a0-8bca-3d126c31849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Model Initialization\n",
    "# ========================\n",
    "\n",
    "send_example_telemetry(\"zeroshot_object_detection_with_owlvit_notebook\", framework=\"pytorch\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "mixin = ImageFeatureExtractionMixin()\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "# ========================\n",
    "# OpenAI API Setup\n",
    "# ========================\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\"))\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",  # Use the appropriate model for embeddings\n",
    "        input=text\n",
    "    )\n",
    "    embeddings = response.data[0].embedding\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Visualization Function\n",
    "# ========================\n",
    "\n",
    "def plot_predictions(input_image, text_queries, scores, boxes, labels, num, act, score_threshold, show=True):\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "  ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "  ax.set_axis_off()\n",
    "\n",
    "  detection_count = 0\n",
    "  for score, box, label in zip(scores, boxes, labels):\n",
    "    detection_count += 1\n",
    "    if score < score_threshold or detection_count > num:\n",
    "      continue\n",
    "    if act == 1:\n",
    "       box = [0.5, 0.5, 1.0, 1.0]\n",
    "    cx, cy, w, h = box\n",
    "    #print(box)\n",
    "    ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n",
    "            [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n",
    "    ax.text(\n",
    "        cx - w / 2,\n",
    "        cy + h / 2 + 0.015,\n",
    "        f\"{text_queries}: {score:1.5f}\",\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        color=\"red\",\n",
    "        bbox={\n",
    "            \"facecolor\": \"white\",\n",
    "            \"edgecolor\": \"red\",\n",
    "            \"boxstyle\": \"square,pad=.3\"\n",
    "        })\n",
    "  if show:\n",
    "      \n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5671ea4-620b-40c1-afa6-145ee4f83530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Image Description Generation Loop\n",
    "# ========================\n",
    "\n",
    "plot_interval = 100  # Plot every 100 images for visualization\n",
    "score_threshold = 0.001\n",
    "\n",
    "for day in ['day1', 'day2', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9']:\n",
    "    print('Generating language descriptions for', day, '...')\n",
    "    print('Querying GPT-4o for image descriptions...')\n",
    "\n",
    "    # Set the data path \n",
    "    #day = 'day3'  \n",
    "    rel_data_path = f'data/{day}/images/'\n",
    "\n",
    "    dir_path = os.path.join(Path.cwd().parent, rel_data_path)\n",
    "    imgs_path = []\n",
    "    for path in os.listdir(dir_path):\n",
    "        imgs_path.append(path)\n",
    "    imgs_path.sort()\n",
    "    descriptions = []\n",
    "\n",
    "    # Load existing dictionaries or initialize empty ones\n",
    "    try:\n",
    "        pickle_file_path = 'image_descriptions_dict.pickle'\n",
    "        with open(pickle_file_path, 'rb') as f:\n",
    "            image_descriptions_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "        pickle_file_path = 'text_embeddings_dict.pickle'\n",
    "        with open(pickle_file_path, 'rb') as f:\n",
    "            text_embeddings_dict = pickle.load(f)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Dictionaries not found. Initializing empty dictionaries.\")\n",
    "        text_embeddings_dict = {}\n",
    "        image_descriptions_dict = {}\n",
    "\n",
    "\n",
    "    for image_index in range(0, len(imgs_path)):\n",
    "        try:\n",
    "            # Image encoding for API\n",
    "            IMAGE_PATH = f'{dir_path}%s'%imgs_path[image_index]\n",
    "            \n",
    "            def encode_image(image_path):\n",
    "                with open(image_path, \"rb\") as image_file:\n",
    "                    return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "            \n",
    "            base64_image = encode_image(IMAGE_PATH)\n",
    "            \n",
    "            # OpenAI API call for image description\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes the images for a robotic vacuum cleaner.\"},\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"You are an AI system that helps a smart robotic vacuum cleaner. This is an image of the floor captured by the camera of the vacuum cleaner robot.\\\n",
    "                        What does the floor look like? in terms of color and pattern. (gray carpet, wooden floor, zigzag carpet, etc.) Keep your answer short.\\\n",
    "                        What do you see on the floor? sprinkles, crumbs, pet feces, nail, chair legs, USB drive, socks\\\n",
    "                        If there is nothing on the floor,just say NONE.\\\n",
    "                        How many of this item is in this image? if it is NONE or uncountable (like crumbs) say 1.\\\n",
    "                        Should the vacuum cleaner suck this item or avoid it? Do not give explanations.\\\n",
    "                        output your answer in this format: item on the floor, avoid/suck, number of item, floor description\\\n",
    "                        examples: \\\n",
    "                        sprinkles, suck, 1, zigzag carpet\\\n",
    "                        NONE, suck, 1, zigzag carpet\\\n",
    "                        crumbs, suck, 1, gray carpet\\\n",
    "                        nail, avoid, 1, wooden floor\\\n",
    "                        paper clip, avoid, 2, wooden floor\\\n",
    "                        NONE, suck, 1, gray carpet\\\n",
    "                        chair leg, avoid, 1, gray carpet\\\n",
    "                    \"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "                        }\n",
    "                    ]}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            descriptions.append(response.choices[0].message.content)\n",
    "            des = response.choices[0].message.content\n",
    "            image_descriptions_dict[f\"{rel_data_path}{imgs_path[image_index]}\"] = des.lower()\n",
    "\n",
    "            item = response.choices[0].message.content\n",
    "            \n",
    "            # Generate embeddings for new items\n",
    "            if item.lower() not in text_embeddings_dict:\n",
    "                text_embeddings_dict[item.lower()] = get_embeddings(item.lower())\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Retrying in 5 seconds...\")\n",
    "            time.sleep(5)  \n",
    "            continue  \n",
    "\n",
    "\n",
    "    # Save generated dictionaries\n",
    "    pickle_file_path = 'image_descriptions_dict.pickle'\n",
    "    with open(pickle_file_path, 'wb') as f:\n",
    "        pickle.dump(image_descriptions_dict, f)\n",
    "    #print(f\"Dictionary saved to {pickle_file_path}\")\n",
    "\n",
    "    pickle_file_path = 'text_embeddings_dict.pickle'\n",
    "    with open(pickle_file_path, 'wb') as f:\n",
    "        pickle.dump(text_embeddings_dict, f)\n",
    "    #print(f\"Dictionary saved to {pickle_file_path}\")\n",
    "\n",
    "    # Create image-to-embedding mapping\n",
    "    image_embeddings_dict = {}\n",
    "    for img_path, description in image_descriptions_dict.items():\n",
    "        if description in text_embeddings_dict:\n",
    "            image_embeddings_dict[img_path] = text_embeddings_dict[description]\n",
    "        else:\n",
    "            print(f\"⚠️  Warning: No embedding found for description: {description}\")\n",
    "\n",
    "\n",
    "\n",
    "    print('Plotting sample images for visualization ... \\n')\n",
    "\n",
    "    imgs_path = sorted(os.listdir(dir_path)) \n",
    "    model = model.to(device)\n",
    "    model.eval()    \n",
    "    \n",
    "    dict_obj = {}\n",
    "    dict_act = {}\n",
    "    dict_floor = {}\n",
    "    dict_num = {}\n",
    "\n",
    "\n",
    "    for key, value in image_descriptions_dict.items():\n",
    "        items = [item.strip() for item in value.split(\",\")]\n",
    "        dict_obj[key] = items[0] if len(items) > 0 else \"\"\n",
    "        dict_act[key] = items[1] if len(items) > 1 else \"\"\n",
    "        dict_num[key] = int(items[2]) if len(items) > 2 else \"\"\n",
    "\n",
    "        dict_floor[key] = items[3] if len(items) > 3 else \"\"\n",
    "\n",
    "\n",
    "\n",
    "    for image_index, img_name in enumerate(imgs_path):\n",
    "\n",
    "\n",
    "        try:\n",
    "            # ------------------------\n",
    "            # Load and preprocess image\n",
    "            # ------------------------\n",
    "            img_path = os.path.join(dir_path, img_name)\n",
    "            image1 = Image.open(img_path)\n",
    "            image = Image.fromarray(np.uint8(image1)).convert(\"RGB\")\n",
    "\n",
    "            text_queries = dict_obj[f\"{rel_data_path}{img_name}\"]\n",
    "\n",
    "            # Prepare inputs\n",
    "            inputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # ------------------------\n",
    "            # Model inference\n",
    "            # ------------------------\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            end = time.time()\n",
    "            #print(f\"Inference time: {end - start:.3f} s\")\n",
    "\n",
    "            # ------------------------\n",
    "            # Post-processing\n",
    "            # ------------------------\n",
    "            image_size = model.config.vision_config.image_size\n",
    "            image_resized = mixin.resize(image, image_size)\n",
    "            input_image = np.asarray(image_resized).astype(np.float32) / 255.0\n",
    "\n",
    "            logits = torch.max(outputs[\"logits\"][0], dim=-1)\n",
    "            scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
    "            labels = logits.indices.cpu().detach().numpy()\n",
    "            boxes = outputs[\"pred_boxes\"][0].cpu().detach().numpy()\n",
    "\n",
    "            # Sort predictions by score\n",
    "            data = list(zip(scores, boxes, labels))\n",
    "            sorted_data = sorted(data, key=lambda x: x[0], reverse=True)\n",
    "            sorted_scores = [d[0] for d in sorted_data]\n",
    "            sorted_boxes = [d[1] for d in sorted_data]\n",
    "            sorted_labels = [d[2] for d in sorted_data]\n",
    "    \n",
    "\n",
    "            # ------------------------\n",
    "            # Plot & Save Labels\n",
    "            # ------------------------\n",
    "            num_objects = dict_num[f\"{rel_data_path}{img_name}\"]\n",
    "            if image_index % plot_interval == 0:\n",
    "                show = True\n",
    "            else:\n",
    "                show = False\n",
    "\n",
    "            act = 0 if 'avoid' in dict_act[f\"{rel_data_path}{img_name}\"] else 1\n",
    "\n",
    "            if image_index % plot_interval == 0:\n",
    "                print('IMAGE PATH:', rel_data_path + img_name)\n",
    "                print('Language description:', image_descriptions_dict[f\"{rel_data_path}{img_name}\"])\n",
    "\n",
    "            plot_predictions(input_image, text_queries, sorted_scores, sorted_boxes, sorted_labels, num_objects, act=act, score_threshold=score_threshold, show=show)\n",
    "            plt.close('all')  # prevent figure accumulation\n",
    "\n",
    "            for box in sorted_boxes[:num_objects]:\n",
    "                rounded_box = [round(num, 5) for num in box]\n",
    "                if image_index % plot_interval == 0:\n",
    "                    print(\"Predicted bounding box:\", rounded_box)\n",
    "            if image_index % plot_interval == 0:\n",
    "                print('------------------------------------------')\n",
    "\n",
    "            \n",
    "\n",
    "            write_data = [(act, sorted_boxes[j]) for j in range(num_objects)]\n",
    "\n",
    "            # Save labels to file\n",
    "            labels_dir = dir_path.replace('images', 'labels')\n",
    "            os.makedirs(labels_dir, exist_ok=True)\n",
    "            label_filename = os.path.join(labels_dir, img_name.replace('.jpg', '.txt'))\n",
    "\n",
    "            with open(label_filename, 'w') as f:\n",
    "                for label, box in write_data:\n",
    "                    if act == 1:\n",
    "                        line = '1.0 0.5 0.5 1.0 1.0\\n'\n",
    "                    else:\n",
    "                        line = f\"{label} {box[0]} {box[1]} {box[2]} {box[3]}\\n\"\n",
    "                    f.write(line)\n",
    "\n",
    "\n",
    "        finally:\n",
    "            # ------------------------\n",
    "            # Cleanup to prevent freezing\n",
    "            # ------------------------\n",
    "            del inputs, outputs, image1, image, image_resized\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa31f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_dict = {}\n",
    "for img_path, description in image_descriptions_dict.items():\n",
    "    if description in text_embeddings_dict:\n",
    "        image_embeddings_dict[img_path] = text_embeddings_dict[description]\n",
    "    else:\n",
    "        print(f\"⚠️  Warning: No embedding found for description: {description}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLM-Vac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
